{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1b010b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc loading | 0.9915254237288136,learning rate:0.1\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc maintenance | 1.0,learning rate:0.1\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#TEIC-module 5.1 (model construction/trained with whole experts data)\n",
    "\n",
    "#enter the filepath2 before running the program\n",
    "\n",
    "filepath1 =\"../dataset/TEIC_data(expert_non-ICU)(TEIC-table3).csv\" #filepath to the dataset (TEIC-table3)\n",
    "filepath2 =\"\" #filepath to the folder where to save the weight parameters\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def one_hot_encoding(x):\n",
    "    unique = list(np.unique(x))\n",
    "    for i in unique:\n",
    "        x = np.where(x == i, unique.index(i), x)\n",
    "    x = np.ravel(x).astype(int)\n",
    "    one_hot = np.eye(len(np.unique(x)))[x]\n",
    "    return unique, one_hot      \n",
    "                                                                                                                       \n",
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x.T\n",
    "        x = x - np.max(x, axis=0)\n",
    "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "        return y.T \n",
    "\n",
    "    x = x - np.max(x) # overflow\n",
    "    return np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.res\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))  \n",
    "\n",
    "def sigmoid_grad(x):\n",
    "    return (1.0 - sigmoid(x)) * sigmoid(x)\n",
    "\n",
    "\n",
    "class TwoLayerNet:\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size).astype(np.float32)\n",
    "        self.params['b1'] = np.zeros(hidden_size).astype(np.float32)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size).astype(np.float32)\n",
    "        self.params['b2'] = np.zeros(output_size).astype(np.float32)\n",
    "        self.h ={}\n",
    "        self.h['W1'] = np.zeros_like(self.params['W1'])\n",
    "        self.h['b1'] = np.zeros_like(self.params['b1'])\n",
    "        self.h['W2'] = np.zeros_like(self.params['W2'])\n",
    "        self.h['b2'] = np.zeros_like(self.params['b2'])\n",
    "\n",
    "\n",
    "    def predict(self, x):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "    \n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        return y\n",
    "        \n",
    "    # x:input, t:output\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        \n",
    "        return cross_entropy_error(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "        \n",
    "    # x:input, t:output\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads\n",
    "        \n",
    "    def gradient(self, x, t):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        grads = {}\n",
    "        \n",
    "        batch_num = x.shape[0]\n",
    "        \n",
    "        # forward\n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        # backward\n",
    "        dy = (y - t) / batch_num\n",
    "        grads['W2'] = np.dot(z1.T, dy)\n",
    "        grads['b2'] = np.sum(dy, axis=0)\n",
    "        \n",
    "        dz1 = np.dot(dy, W2.T)\n",
    "        da1 = sigmoid_grad(a1) * dz1\n",
    "        grads['W1'] = np.dot(x.T, da1)\n",
    "        grads['b1'] = np.sum(da1, axis=0)\n",
    "\n",
    "        return grads\n",
    "\n",
    "data = pd.read_csv(filepath1)\n",
    "ID = np.array(data[[\"index\"]])\n",
    "param_list1=[\"Age\",\"body weight\",\"BMI\",\"Creatinine clearance\",\"Alb\",\"TO\"]\n",
    "param_list2 = [\"T1\",\"T2\",\"T3\",\"T4\",\"T5\",\"T6\",\"T7\"]\n",
    "parameter1 = np.array(data[param_list1]).astype(np.float32) #patameter1\n",
    "parameter2 = np.array(data[param_list2]) #patameter2\n",
    "loading = np.array(data[[\"loading dose\"]])#loading\n",
    "loading_dose = np.copy(loading)\n",
    "maintenance = np.array(data[[\"maintenance dose\"]]) #maintenance\n",
    "maintenance_dose = np.copy(maintenance)\n",
    "TDM = np.array(data[[\"TDM\",\"TDM exclusion criteria\"]])\n",
    "\n",
    "#transform loading to one-hot encoding\n",
    "ll, loading_one_hot = one_hot_encoding(loading)\n",
    "\n",
    "\n",
    "#transform maintenance to one-hot encoding\n",
    "lm, maintenance_one_hot = one_hot_encoding(maintenance)\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "sc.fit(parameter1)\n",
    "parameter_scale= sc.transform(parameter1)\n",
    "parameter = np.concatenate([parameter_scale, parameter2], 1)  \n",
    "\n",
    "#deep learning for loading\n",
    "network_loading = TwoLayerNet(input_size=len(param_list1)+len(param_list2), hidden_size=15, output_size=len(np.unique(loading)))\n",
    "\n",
    "iters_num = 2000  #iteration\n",
    "j=0.1 #learning rate\n",
    "learning_rate = j\n",
    "train_loss_loading_list = []\n",
    "train_acc_loading_list = []\n",
    "for i in range(iters_num):\n",
    "    #grads = network.numerical_gradient(x_batch, t_batch)\n",
    "    grads = network_loading.gradient(parameter, loading_one_hot)\n",
    "    #parameter update\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network_loading.h[key] += grads[key] * grads[key]\n",
    "        network_loading.params[key] -= learning_rate * grads[key]/(np.sqrt(network_loading.h[key])+1e-7)\n",
    "    loss = network_loading.loss(parameter, loading_one_hot)\n",
    "    train_loss_loading_list.append(loss)    \n",
    "    train_acc_loading = network_loading.accuracy(parameter, loading_one_hot)\n",
    "    train_acc_loading_list.append(train_acc_loading)\n",
    "np.save(str(filepath2)+\"/loading.param(W1_whole_non-ICU_lr:{}).npy\".format(j), network_loading.params[\"W1\"])\n",
    "np.save(str(filepath2)+\"/loading.param(b1_whole_non-ICU_lr:{}).npy\".format(j), network_loading.params[\"b1\"])\n",
    "np.save(str(filepath2)+\"/loading.param(W2_whole_non-ICU_lr:{}).npy\".format(j), network_loading.params[\"W2\"])\n",
    "np.save(str(filepath2)+\"/loading.param(b2_whole_non-ICU_lr:{}).npy\".format(j), network_loading.params[\"b2\"])\n",
    "print(\"train acc loading | \" + str(train_acc_loading)+ \",\" +\"learning rate:\"+str(j))\n",
    "#graph\n",
    "x = np.arange(len(train_acc_loading_list))\n",
    "plt.plot(x, train_acc_loading_list, label='train acc loading_lr:{}'.format(j))\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.show()\n",
    "\n",
    "#deep learning for maintenance\n",
    "network_maintenance = TwoLayerNet(input_size=len(param_list1)+len(param_list2), hidden_size=15, output_size=len(np.unique(maintenance)))\n",
    "\n",
    "iters_num = 2000  #iteration\n",
    "\n",
    "j = 0.1 #learning rate\n",
    "learning_rate = j\n",
    "train_loss_maintenance_list = []\n",
    "train_acc_maintenance_list = []\n",
    "for i in range(iters_num):\n",
    "    #grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    grads = network_maintenance.gradient(parameter, maintenance_one_hot)\n",
    "    #parameter update\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network_maintenance.h[key] += grads[key] * grads[key]\n",
    "        network_maintenance.params[key] -= learning_rate * grads[key]/(np.sqrt(network_maintenance.h[key])+1e-7)\n",
    "    loss = network_maintenance.loss(parameter, maintenance_one_hot)\n",
    "    train_loss_maintenance_list.append(loss)\n",
    "    train_acc_maintenance = network_maintenance.accuracy(parameter, maintenance_one_hot)\n",
    "    train_acc_maintenance_list.append(train_acc_maintenance)\n",
    "np.save(str(filepath2)+\"/maintenance.param(W1_whole_non-ICU_lr:{}).npy\".format(j), network_maintenance.params[\"W1\"])\n",
    "np.save(str(filepath2)+\"/maintenance.param(b1_whole_non-ICU_lr:{}).npy\".format(j), network_maintenance.params[\"b1\"])\n",
    "np.save(str(filepath2)+\"/maintenance.param(W2_whole_non-ICU_lr:{}).npy\".format(j), network_maintenance.params[\"W2\"])\n",
    "np.save(str(filepath2)+\"/maintenance.param(b2_whole_non-ICU_lr:{}).npy\".format(j), network_maintenance.params[\"b2\"])\n",
    "print(\"train acc maintenance | \" + str(train_acc_maintenance) + \",\" +\"learning rate:\"+str(j))\n",
    "#graph\n",
    "x = np.arange(len(train_acc_maintenance_list))\n",
    "plt.plot(x, train_acc_maintenance_list, label='train acc maintenance_lr:{}'.format(j))\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cb76f4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Age</th>\n",
       "      <th>body weight</th>\n",
       "      <th>BMI</th>\n",
       "      <th>Creatinine clearance</th>\n",
       "      <th>Alb</th>\n",
       "      <th>TO</th>\n",
       "      <th>T1</th>\n",
       "      <th>T2</th>\n",
       "      <th>T3</th>\n",
       "      <th>T4</th>\n",
       "      <th>T5</th>\n",
       "      <th>T6</th>\n",
       "      <th>T7</th>\n",
       "      <th>loading dose</th>\n",
       "      <th>pred_loading dose</th>\n",
       "      <th>maintenance dose</th>\n",
       "      <th>pred_maintenance dose</th>\n",
       "      <th>TDM</th>\n",
       "      <th>TDM exclusion criteria</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>72.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>18.561278</td>\n",
       "      <td>76.455026</td>\n",
       "      <td>1.9</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>400mg 4 doses</td>\n",
       "      <td>400mg 4 doses</td>\n",
       "      <td>400 mg</td>\n",
       "      <td>0 mg</td>\n",
       "      <td>15.3</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>77.0</td>\n",
       "      <td>68.5</td>\n",
       "      <td>25.62448</td>\n",
       "      <td>43.75</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>800mg 3 doses</td>\n",
       "      <td>600mg 4 doses</td>\n",
       "      <td>300 mg</td>\n",
       "      <td>0 mg</td>\n",
       "      <td>26.7</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>79.0</td>\n",
       "      <td>39.8</td>\n",
       "      <td>15.782728</td>\n",
       "      <td>35.82691</td>\n",
       "      <td>2.6</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>400mg 3 doses</td>\n",
       "      <td>400mg 3 doses</td>\n",
       "      <td>200 mg</td>\n",
       "      <td>0 mg</td>\n",
       "      <td>9.7</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21</td>\n",
       "      <td>71.0</td>\n",
       "      <td>46.3</td>\n",
       "      <td>20.854916</td>\n",
       "      <td>89.798115</td>\n",
       "      <td>2.5</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>400mg 2 doses</td>\n",
       "      <td>400mg 4 doses</td>\n",
       "      <td>200 mg</td>\n",
       "      <td>400 mg</td>\n",
       "      <td>9.8</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22</td>\n",
       "      <td>72.0</td>\n",
       "      <td>49.2</td>\n",
       "      <td>22.280631</td>\n",
       "      <td>101.273504</td>\n",
       "      <td>3.5</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>400mg 3 doses</td>\n",
       "      <td>600mg 4 doses</td>\n",
       "      <td>0 mg</td>\n",
       "      <td>0 mg</td>\n",
       "      <td>11.8</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>175</td>\n",
       "      <td>74.0</td>\n",
       "      <td>68.5</td>\n",
       "      <td>22.755774</td>\n",
       "      <td>120.753205</td>\n",
       "      <td>2.2</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>700mg 2 doses</td>\n",
       "      <td>600mg 4 doses</td>\n",
       "      <td>500 mg</td>\n",
       "      <td>0 mg</td>\n",
       "      <td>14.1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>176</td>\n",
       "      <td>50.0</td>\n",
       "      <td>63.4</td>\n",
       "      <td>22.224396</td>\n",
       "      <td>90.056818</td>\n",
       "      <td>3.3</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>600mg 4 doses</td>\n",
       "      <td>600mg 4 doses</td>\n",
       "      <td>400 mg</td>\n",
       "      <td>0 mg</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>179</td>\n",
       "      <td>67.0</td>\n",
       "      <td>60.3</td>\n",
       "      <td>23.881935</td>\n",
       "      <td>117.572115</td>\n",
       "      <td>2.7</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>400mg 3 doses</td>\n",
       "      <td>600mg 4 doses</td>\n",
       "      <td>400 mg</td>\n",
       "      <td>500 mg</td>\n",
       "      <td>15.4</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>180</td>\n",
       "      <td>77.0</td>\n",
       "      <td>52.1</td>\n",
       "      <td>22.910525</td>\n",
       "      <td>25.326389</td>\n",
       "      <td>2.9</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>400mg 1 doses</td>\n",
       "      <td>400mg 4 doses</td>\n",
       "      <td>0 mg</td>\n",
       "      <td>200 mg</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>181</td>\n",
       "      <td>82.0</td>\n",
       "      <td>66.9</td>\n",
       "      <td>24.72261</td>\n",
       "      <td>48.992424</td>\n",
       "      <td>3.5</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>400mg 4 doses</td>\n",
       "      <td>600mg 4 doses</td>\n",
       "      <td>0 mg</td>\n",
       "      <td>0 mg</td>\n",
       "      <td>19.4</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>81 rows Ã— 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   index   Age body weight        BMI Creatinine clearance  Alb    TO T1 T2  \\\n",
       "0      2  72.0        40.0  18.561278            76.455026  1.9  10.0  0  0   \n",
       "1      6  77.0        68.5   25.62448                43.75  3.0  10.0  0  1   \n",
       "2     10  79.0        39.8  15.782728             35.82691  2.6  14.0  0  0   \n",
       "3     21  71.0        46.3  20.854916            89.798115  2.5   8.0  1  0   \n",
       "4     22  72.0        49.2  22.280631           101.273504  3.5  15.0  1  0   \n",
       "..   ...   ...         ...        ...                  ...  ...   ... .. ..   \n",
       "76   175  74.0        68.5  22.755774           120.753205  2.2  13.0  0  0   \n",
       "77   176  50.0        63.4  22.224396            90.056818  3.3  20.0  0  0   \n",
       "78   179  67.0        60.3  23.881935           117.572115  2.7  12.0  0  0   \n",
       "79   180  77.0        52.1  22.910525            25.326389  2.9   9.0  0  0   \n",
       "80   181  82.0        66.9   24.72261            48.992424  3.5  18.0  0  0   \n",
       "\n",
       "   T3 T4 T5 T6 T7   loading dose pred_loading dose maintenance dose  \\\n",
       "0   1  0  0  0  0  400mg 4 doses     400mg 4 doses           400 mg   \n",
       "1   0  0  0  0  0  800mg 3 doses     600mg 4 doses           300 mg   \n",
       "2   1  0  0  0  0  400mg 3 doses     400mg 3 doses           200 mg   \n",
       "3   0  0  0  0  0  400mg 2 doses     400mg 4 doses           200 mg   \n",
       "4   0  0  0  0  0  400mg 3 doses     600mg 4 doses             0 mg   \n",
       ".. .. .. .. .. ..            ...               ...              ...   \n",
       "76  0  0  0  0  1  700mg 2 doses     600mg 4 doses           500 mg   \n",
       "77  0  0  0  0  1  600mg 4 doses     600mg 4 doses           400 mg   \n",
       "78  0  0  1  0  0  400mg 3 doses     600mg 4 doses           400 mg   \n",
       "79  0  0  1  0  0  400mg 1 doses     400mg 4 doses             0 mg   \n",
       "80  0  0  0  1  0  400mg 4 doses     600mg 4 doses             0 mg   \n",
       "\n",
       "   pred_maintenance dose   TDM TDM exclusion criteria  \n",
       "0                   0 mg  15.3                    0.0  \n",
       "1                   0 mg  26.7                    0.0  \n",
       "2                   0 mg   9.7                    0.0  \n",
       "3                 400 mg   9.8                    0.0  \n",
       "4                   0 mg  11.8                    0.0  \n",
       "..                   ...   ...                    ...  \n",
       "76                  0 mg  14.1                    0.0  \n",
       "77                  0 mg  21.0                    0.0  \n",
       "78                500 mg  15.4                    0.0  \n",
       "79                200 mg   6.0                    0.0  \n",
       "80                  0 mg  19.4                    0.0  \n",
       "\n",
       "[81 rows x 20 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#TEIC-module 5.2 (model evaluation)\n",
    "\n",
    "#enter the filepath4 before running the program\n",
    "\n",
    "filepath1 =\"../dataset/TEIC_data(non-expert_non-ICU)(TEIC-table11).csv\" #filepath to the non-expert dataset (TEIC-table11)\n",
    "filepath2 =\"../dataset/TEIC_data(expert_non-ICU)(TEIC-table3).csv\" #filepath to the expert dataset(TEIC-table3)\n",
    "filepath3 =\"../weight parameter/external validation (expertML)\" #filepath to the folder containing weight parameters\n",
    "filepath4 =\"\" #filepath to the folder where to save the result\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def one_hot_encoding(x):\n",
    "    unique = list(np.unique(x))\n",
    "    for i in unique:\n",
    "        x = np.where(x == i, unique.index(i), x)\n",
    "    x = np.ravel(x).astype(int)\n",
    "    one_hot = np.eye(len(np.unique(x)))[x]\n",
    "    return unique, one_hot      \n",
    "                                                                                                                                                                                                       \n",
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x.T\n",
    "        x = x - np.max(x, axis=0)\n",
    "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "        return y.T \n",
    "\n",
    "    x = x - np.max(x) #overflow\n",
    "    return np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.res\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))  \n",
    "\n",
    "def sigmoid_grad(x):\n",
    "    return (1.0 - sigmoid(x)) * sigmoid(x)\n",
    "\n",
    "\n",
    "class TwoLayerNet:\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "    def predict(self, x):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "    \n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        return y\n",
    "        \n",
    "    # x:input, t:output\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        \n",
    "        return cross_entropy_error(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "        \n",
    "    # x:input, t:output\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads\n",
    "        \n",
    "    def gradient(self, x, t):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        grads = {}\n",
    "        \n",
    "        batch_num = x.shape[0]\n",
    "        \n",
    "        # forward\n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        # backward\n",
    "        dy = (y - t) / batch_num\n",
    "        grads['W2'] = np.dot(z1.T, dy)\n",
    "        grads['b2'] = np.sum(dy, axis=0)\n",
    "        \n",
    "        dz1 = np.dot(dy, W2.T)\n",
    "        da1 = sigmoid_grad(a1) * dz1\n",
    "        grads['W1'] = np.dot(x.T, da1)\n",
    "        grads['b1'] = np.sum(da1, axis=0)\n",
    "\n",
    "        return grads\n",
    "\n",
    "data = pd.read_csv(filepath1)#non-expert data\n",
    "data_2 = pd.read_csv(filepath2)#expert data\n",
    "\n",
    "param_list1=[\"Age\",\"body weight\",\"BMI\",\"Creatinine clearance\",\"Alb\",\"TO\"]\n",
    "param_list2 = [\"T1\",\"T2\",\"T3\",\"T4\",\"T5\",\"T6\",\"T7\"]\n",
    "param_list3 = [\"index\"]\n",
    "\n",
    "parameter1 = np.array(data[param_list1]) #patameter1 in non-expert (numerical)\n",
    "parameter2 = np.array(data[param_list2]) #patameter2 in non-expert (binary)\n",
    "parameter_model=np.array(data_2[param_list1]) #parameter in expert(numerical)\n",
    "ID = np.array(data[param_list3])\n",
    "loading_model = np.array(data_2[[\"loading dose\"]])#experts' loading\n",
    "maintenance_model = np.array(data_2[[\"maintenance dose\"]]) #experts' maintenance\n",
    "loading = np.array(data[[\"loading dose\"]])\n",
    "maintenance = np.array(data[[\"maintenance dose\"]])\n",
    "TDM = np.array(data[[\"TDM\",\"TDM exclusion criteria\"]])\n",
    "\n",
    "#transform loading to one-hot encoding\n",
    "ll, loading_one_hot = one_hot_encoding(loading_model)\n",
    "\n",
    "#transform maintenance to one-hot encoding\n",
    "lm, maintenance_one_hot = one_hot_encoding(maintenance_model)\n",
    "\n",
    "\n",
    "sc = StandardScaler()\n",
    "sc.fit(parameter_model) #parameter in expert (numerical)\n",
    "parameter1_scaler = sc.transform(parameter1)\n",
    "parameter_scaler = np.concatenate([parameter1_scaler, parameter2], 1)\n",
    "\n",
    "#loading\n",
    "network_loading = TwoLayerNet(input_size=len(param_list1)+len(param_list2), hidden_size=15, output_size=len(np.unique(loading_model)))\n",
    "\n",
    "#parameter incorporation\n",
    "for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "    network_loading.params[key] = np.load(str(filepath3)+\"/loading.param({}_whole_non-ICU_lr_0.1).npy\".format(key))\n",
    "    \n",
    "\n",
    "pred_loading_one_hot_to_index = list(np.argmax(network_loading.predict(parameter_scaler),axis=1))\n",
    "pred_loading_dose = np.array([ll[i] for i in pred_loading_one_hot_to_index]).reshape((len(pred_loading_one_hot_to_index), 1))\n",
    "\n",
    "\n",
    "#maintenance\n",
    "network_maintenance = TwoLayerNet(input_size=len(param_list1)+len(param_list2), hidden_size=15, output_size=len(np.unique(maintenance_model)))\n",
    "\n",
    "#parameter incorporation\n",
    "for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "    network_maintenance.params[key] = np.load(str(filepath3)+\"/maintenance.param({}_whole_non-ICU_lr_0.1).npy\".format(key))\n",
    "    \n",
    "\n",
    "pred_maintenance_one_hot_to_index = list(np.argmax(network_maintenance.predict(parameter_scaler),axis=1))\n",
    "pred_maintenance_dose = np.array([lm[i] for i in pred_maintenance_one_hot_to_index]).reshape(len(pred_maintenance_one_hot_to_index), 1)\n",
    "\n",
    "np_result = np.concatenate([ID, parameter1,parameter2, loading, pred_loading_dose,maintenance,pred_maintenance_dose, TDM],1)\n",
    "df_list = param_list3+param_list1+param_list2+[\"loading dose\",\"pred_loading dose\",\"maintenance dose\",\"pred_maintenance dose\",\"TDM\",\"TDM exclusion criteria\"]\n",
    "df_result = pd.DataFrame(data = np_result, columns= df_list)\n",
    "display(df_result)\n",
    "df_result.to_csv(str(filepath4)+\"/TEIC_result_non-ICU(non-expert_validation).csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2b290f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "permuted importance(loading); mean and stdev\n",
      "({'Age': 0.22627118644067795, 'body weight': 0.5084745762711864, 'BMI': 0.3377118644067797, 'Creatinine clearance': 0.30847457627118646, 'Alb': 0.25677966101694916, 'TO': 0.3114406779661017, 'T1': 0.14067796610169492, 'T2': 0.1775423728813559, 'T3': 0.18516949152542372, 'T4': 0.14915254237288136, 'T5': 0.05254237288135592, 'T6': 0.011440677966101664, 'T7': 0.0016949152542372835}, {'Age': 0.03334744204155709, 'body weight': 0.027218815882948007, 'BMI': 0.0439299119180809, 'Creatinine clearance': 0.031875413286889975, 'Alb': 0.028980990166243302, 'TO': 0.03551827205322573, 'T1': 0.02306973041961764, 'T2': 0.034612787035773765, 'T3': 0.023855118164342718, 'T4': 0.025408856786072277, 'T5': 0.01363715395996286, 'T6': 0.004147122753640607, 'T7': 0.003477892718932716})\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>body weight</th>\n",
       "      <th>BMI</th>\n",
       "      <th>Creatinine clearance</th>\n",
       "      <th>Alb</th>\n",
       "      <th>TO</th>\n",
       "      <th>T1</th>\n",
       "      <th>T2</th>\n",
       "      <th>T3</th>\n",
       "      <th>T4</th>\n",
       "      <th>T5</th>\n",
       "      <th>T6</th>\n",
       "      <th>T7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.228814</td>\n",
       "      <td>0.508475</td>\n",
       "      <td>0.355932</td>\n",
       "      <td>0.271186</td>\n",
       "      <td>0.245763</td>\n",
       "      <td>0.29661</td>\n",
       "      <td>0.161017</td>\n",
       "      <td>0.152542</td>\n",
       "      <td>0.177966</td>\n",
       "      <td>0.194915</td>\n",
       "      <td>0.059322</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.228814</td>\n",
       "      <td>0.508475</td>\n",
       "      <td>0.415254</td>\n",
       "      <td>0.330508</td>\n",
       "      <td>0.237288</td>\n",
       "      <td>0.338983</td>\n",
       "      <td>0.186441</td>\n",
       "      <td>0.20339</td>\n",
       "      <td>0.186441</td>\n",
       "      <td>0.177966</td>\n",
       "      <td>0.059322</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.245763</td>\n",
       "      <td>0.567797</td>\n",
       "      <td>0.347458</td>\n",
       "      <td>0.305085</td>\n",
       "      <td>0.262712</td>\n",
       "      <td>0.29661</td>\n",
       "      <td>0.144068</td>\n",
       "      <td>0.194915</td>\n",
       "      <td>0.161017</td>\n",
       "      <td>0.152542</td>\n",
       "      <td>0.050847</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.228814</td>\n",
       "      <td>0.550847</td>\n",
       "      <td>0.372881</td>\n",
       "      <td>0.338983</td>\n",
       "      <td>0.237288</td>\n",
       "      <td>0.355932</td>\n",
       "      <td>0.127119</td>\n",
       "      <td>0.161017</td>\n",
       "      <td>0.177966</td>\n",
       "      <td>0.177966</td>\n",
       "      <td>0.059322</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.271186</td>\n",
       "      <td>0.483051</td>\n",
       "      <td>0.313559</td>\n",
       "      <td>0.330508</td>\n",
       "      <td>0.254237</td>\n",
       "      <td>0.279661</td>\n",
       "      <td>0.110169</td>\n",
       "      <td>0.144068</td>\n",
       "      <td>0.177966</td>\n",
       "      <td>0.144068</td>\n",
       "      <td>0.059322</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.008475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.254237</td>\n",
       "      <td>0.533898</td>\n",
       "      <td>0.355932</td>\n",
       "      <td>0.322034</td>\n",
       "      <td>0.288136</td>\n",
       "      <td>0.29661</td>\n",
       "      <td>0.161017</td>\n",
       "      <td>0.237288</td>\n",
       "      <td>0.211864</td>\n",
       "      <td>0.144068</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.194915</td>\n",
       "      <td>0.483051</td>\n",
       "      <td>0.288136</td>\n",
       "      <td>0.364407</td>\n",
       "      <td>0.220339</td>\n",
       "      <td>0.29661</td>\n",
       "      <td>0.169492</td>\n",
       "      <td>0.20339</td>\n",
       "      <td>0.177966</td>\n",
       "      <td>0.135593</td>\n",
       "      <td>0.059322</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.152542</td>\n",
       "      <td>0.491525</td>\n",
       "      <td>0.305085</td>\n",
       "      <td>0.330508</td>\n",
       "      <td>0.245763</td>\n",
       "      <td>0.330508</td>\n",
       "      <td>0.127119</td>\n",
       "      <td>0.144068</td>\n",
       "      <td>0.169492</td>\n",
       "      <td>0.127119</td>\n",
       "      <td>0.059322</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.262712</td>\n",
       "      <td>0.508475</td>\n",
       "      <td>0.364407</td>\n",
       "      <td>0.288136</td>\n",
       "      <td>0.271186</td>\n",
       "      <td>0.330508</td>\n",
       "      <td>0.135593</td>\n",
       "      <td>0.186441</td>\n",
       "      <td>0.194915</td>\n",
       "      <td>0.161017</td>\n",
       "      <td>0.042373</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.211864</td>\n",
       "      <td>0.508475</td>\n",
       "      <td>0.355932</td>\n",
       "      <td>0.237288</td>\n",
       "      <td>0.245763</td>\n",
       "      <td>0.254237</td>\n",
       "      <td>0.135593</td>\n",
       "      <td>0.194915</td>\n",
       "      <td>0.177966</td>\n",
       "      <td>0.169492</td>\n",
       "      <td>0.042373</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.237288</td>\n",
       "      <td>0.516949</td>\n",
       "      <td>0.322034</td>\n",
       "      <td>0.271186</td>\n",
       "      <td>0.262712</td>\n",
       "      <td>0.381356</td>\n",
       "      <td>0.101695</td>\n",
       "      <td>0.127119</td>\n",
       "      <td>0.220339</td>\n",
       "      <td>0.177966</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.245763</td>\n",
       "      <td>0.550847</td>\n",
       "      <td>0.381356</td>\n",
       "      <td>0.29661</td>\n",
       "      <td>0.262712</td>\n",
       "      <td>0.245763</td>\n",
       "      <td>0.127119</td>\n",
       "      <td>0.161017</td>\n",
       "      <td>0.161017</td>\n",
       "      <td>0.135593</td>\n",
       "      <td>0.076271</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.211864</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.279661</td>\n",
       "      <td>0.347458</td>\n",
       "      <td>0.211864</td>\n",
       "      <td>0.322034</td>\n",
       "      <td>0.118644</td>\n",
       "      <td>0.169492</td>\n",
       "      <td>0.161017</td>\n",
       "      <td>0.101695</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.262712</td>\n",
       "      <td>0.466102</td>\n",
       "      <td>0.322034</td>\n",
       "      <td>0.288136</td>\n",
       "      <td>0.288136</td>\n",
       "      <td>0.313559</td>\n",
       "      <td>0.127119</td>\n",
       "      <td>0.135593</td>\n",
       "      <td>0.211864</td>\n",
       "      <td>0.169492</td>\n",
       "      <td>0.059322</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.008475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.245763</td>\n",
       "      <td>0.474576</td>\n",
       "      <td>0.29661</td>\n",
       "      <td>0.355932</td>\n",
       "      <td>0.305085</td>\n",
       "      <td>0.355932</td>\n",
       "      <td>0.144068</td>\n",
       "      <td>0.228814</td>\n",
       "      <td>0.194915</td>\n",
       "      <td>0.110169</td>\n",
       "      <td>0.042373</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.186441</td>\n",
       "      <td>0.474576</td>\n",
       "      <td>0.288136</td>\n",
       "      <td>0.313559</td>\n",
       "      <td>0.254237</td>\n",
       "      <td>0.288136</td>\n",
       "      <td>0.135593</td>\n",
       "      <td>0.135593</td>\n",
       "      <td>0.152542</td>\n",
       "      <td>0.152542</td>\n",
       "      <td>0.050847</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.20339</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.432203</td>\n",
       "      <td>0.288136</td>\n",
       "      <td>0.237288</td>\n",
       "      <td>0.338983</td>\n",
       "      <td>0.161017</td>\n",
       "      <td>0.144068</td>\n",
       "      <td>0.20339</td>\n",
       "      <td>0.161017</td>\n",
       "      <td>0.042373</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.211864</td>\n",
       "      <td>0.525424</td>\n",
       "      <td>0.279661</td>\n",
       "      <td>0.29661</td>\n",
       "      <td>0.271186</td>\n",
       "      <td>0.330508</td>\n",
       "      <td>0.110169</td>\n",
       "      <td>0.211864</td>\n",
       "      <td>0.161017</td>\n",
       "      <td>0.152542</td>\n",
       "      <td>0.076271</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.169492</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.364407</td>\n",
       "      <td>0.305085</td>\n",
       "      <td>0.322034</td>\n",
       "      <td>0.313559</td>\n",
       "      <td>0.169492</td>\n",
       "      <td>0.228814</td>\n",
       "      <td>0.245763</td>\n",
       "      <td>0.118644</td>\n",
       "      <td>0.050847</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.271186</td>\n",
       "      <td>0.516949</td>\n",
       "      <td>0.313559</td>\n",
       "      <td>0.288136</td>\n",
       "      <td>0.211864</td>\n",
       "      <td>0.262712</td>\n",
       "      <td>0.161017</td>\n",
       "      <td>0.186441</td>\n",
       "      <td>0.177966</td>\n",
       "      <td>0.118644</td>\n",
       "      <td>0.067797</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Age body weight       BMI Creatinine clearance       Alb        TO  \\\n",
       "0   0.228814    0.508475  0.355932             0.271186  0.245763   0.29661   \n",
       "1   0.228814    0.508475  0.415254             0.330508  0.237288  0.338983   \n",
       "2   0.245763    0.567797  0.347458             0.305085  0.262712   0.29661   \n",
       "3   0.228814    0.550847  0.372881             0.338983  0.237288  0.355932   \n",
       "4   0.271186    0.483051  0.313559             0.330508  0.254237  0.279661   \n",
       "5   0.254237    0.533898  0.355932             0.322034  0.288136   0.29661   \n",
       "6   0.194915    0.483051  0.288136             0.364407  0.220339   0.29661   \n",
       "7   0.152542    0.491525  0.305085             0.330508  0.245763  0.330508   \n",
       "8   0.262712    0.508475  0.364407             0.288136  0.271186  0.330508   \n",
       "9   0.211864    0.508475  0.355932             0.237288  0.245763  0.254237   \n",
       "10  0.237288    0.516949  0.322034             0.271186  0.262712  0.381356   \n",
       "11  0.245763    0.550847  0.381356              0.29661  0.262712  0.245763   \n",
       "12  0.211864         0.5  0.279661             0.347458  0.211864  0.322034   \n",
       "13  0.262712    0.466102  0.322034             0.288136  0.288136  0.313559   \n",
       "14  0.245763    0.474576   0.29661             0.355932  0.305085  0.355932   \n",
       "15  0.186441    0.474576  0.288136             0.313559  0.254237  0.288136   \n",
       "16   0.20339         0.5  0.432203             0.288136  0.237288  0.338983   \n",
       "17  0.211864    0.525424  0.279661              0.29661  0.271186  0.330508   \n",
       "18  0.169492         0.5  0.364407             0.305085  0.322034  0.313559   \n",
       "19  0.271186    0.516949  0.313559             0.288136  0.211864  0.262712   \n",
       "\n",
       "          T1        T2        T3        T4        T5        T6        T7  \n",
       "0   0.161017  0.152542  0.177966  0.194915  0.059322  0.008475       0.0  \n",
       "1   0.186441   0.20339  0.186441  0.177966  0.059322  0.008475       0.0  \n",
       "2   0.144068  0.194915  0.161017  0.152542  0.050847  0.008475       0.0  \n",
       "3   0.127119  0.161017  0.177966  0.177966  0.059322  0.016949       0.0  \n",
       "4   0.110169  0.144068  0.177966  0.144068  0.059322  0.016949  0.008475  \n",
       "5   0.161017  0.237288  0.211864  0.144068  0.033898  0.008475       0.0  \n",
       "6   0.169492   0.20339  0.177966  0.135593  0.059322  0.008475       0.0  \n",
       "7   0.127119  0.144068  0.169492  0.127119  0.059322  0.016949       0.0  \n",
       "8   0.135593  0.186441  0.194915  0.161017  0.042373  0.008475       0.0  \n",
       "9   0.135593  0.194915  0.177966  0.169492  0.042373  0.016949       0.0  \n",
       "10  0.101695  0.127119  0.220339  0.177966  0.033898  0.008475  0.008475  \n",
       "11  0.127119  0.161017  0.161017  0.135593  0.076271  0.008475       0.0  \n",
       "12  0.118644  0.169492  0.161017  0.101695  0.025424  0.016949       0.0  \n",
       "13  0.127119  0.135593  0.211864  0.169492  0.059322  0.016949  0.008475  \n",
       "14  0.144068  0.228814  0.194915  0.110169  0.042373  0.008475       0.0  \n",
       "15  0.135593  0.135593  0.152542  0.152542  0.050847  0.008475  0.008475  \n",
       "16  0.161017  0.144068   0.20339  0.161017  0.042373  0.016949       0.0  \n",
       "17  0.110169  0.211864  0.161017  0.152542  0.076271  0.008475       0.0  \n",
       "18  0.169492  0.228814  0.245763  0.118644  0.050847  0.008475       0.0  \n",
       "19  0.161017  0.186441  0.177966  0.118644  0.067797  0.008475       0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "permuted importance (maintenance); mean and stdev\n",
      "({'Age': 0.1775423728813559, 'body weight': 0.3783898305084746, 'BMI': 0.34152542372881356, 'Creatinine clearance': 0.33008474576271185, 'Alb': 0.25084745762711863, 'TO': 0.21779661016949153, 'T1': 0.0944915254237288, 'T2': 0.1563559322033898, 'T3': 0.2135593220338983, 'T4': 0.17627118644067796, 'T5': 0.05805084745762712, 'T6': 0.0021186440677966045, 'T7': 0.0016949152542372835}, {'Age': 0.02615293810256515, 'body weight': 0.04235280523508197, 'BMI': 0.04069840402415365, 'Creatinine clearance': 0.037242998388323414, 'Alb': 0.03449519100647754, 'TO': 0.03500641987202218, 'T1': 0.022551895126856354, 'T2': 0.021905720636908196, 'T3': 0.02891570287861796, 'T4': 0.020867356313596343, 'T5': 0.016330132226183063, 'T6': 0.003764929307790831, 'T7': 0.003477892718932716})\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>body weight</th>\n",
       "      <th>BMI</th>\n",
       "      <th>Creatinine clearance</th>\n",
       "      <th>Alb</th>\n",
       "      <th>TO</th>\n",
       "      <th>T1</th>\n",
       "      <th>T2</th>\n",
       "      <th>T3</th>\n",
       "      <th>T4</th>\n",
       "      <th>T5</th>\n",
       "      <th>T6</th>\n",
       "      <th>T7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.161017</td>\n",
       "      <td>0.491525</td>\n",
       "      <td>0.389831</td>\n",
       "      <td>0.389831</td>\n",
       "      <td>0.271186</td>\n",
       "      <td>0.228814</td>\n",
       "      <td>0.09322</td>\n",
       "      <td>0.144068</td>\n",
       "      <td>0.194915</td>\n",
       "      <td>0.194915</td>\n",
       "      <td>0.067797</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.186441</td>\n",
       "      <td>0.372881</td>\n",
       "      <td>0.415254</td>\n",
       "      <td>0.398305</td>\n",
       "      <td>0.237288</td>\n",
       "      <td>0.271186</td>\n",
       "      <td>0.067797</td>\n",
       "      <td>0.169492</td>\n",
       "      <td>0.186441</td>\n",
       "      <td>0.169492</td>\n",
       "      <td>0.050847</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.20339</td>\n",
       "      <td>0.389831</td>\n",
       "      <td>0.279661</td>\n",
       "      <td>0.322034</td>\n",
       "      <td>0.254237</td>\n",
       "      <td>0.220339</td>\n",
       "      <td>0.059322</td>\n",
       "      <td>0.161017</td>\n",
       "      <td>0.144068</td>\n",
       "      <td>0.177966</td>\n",
       "      <td>0.050847</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.186441</td>\n",
       "      <td>0.423729</td>\n",
       "      <td>0.322034</td>\n",
       "      <td>0.398305</td>\n",
       "      <td>0.254237</td>\n",
       "      <td>0.220339</td>\n",
       "      <td>0.135593</td>\n",
       "      <td>0.144068</td>\n",
       "      <td>0.211864</td>\n",
       "      <td>0.161017</td>\n",
       "      <td>0.084746</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.161017</td>\n",
       "      <td>0.355932</td>\n",
       "      <td>0.305085</td>\n",
       "      <td>0.313559</td>\n",
       "      <td>0.313559</td>\n",
       "      <td>0.254237</td>\n",
       "      <td>0.050847</td>\n",
       "      <td>0.127119</td>\n",
       "      <td>0.211864</td>\n",
       "      <td>0.152542</td>\n",
       "      <td>0.067797</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.127119</td>\n",
       "      <td>0.415254</td>\n",
       "      <td>0.338983</td>\n",
       "      <td>0.305085</td>\n",
       "      <td>0.29661</td>\n",
       "      <td>0.211864</td>\n",
       "      <td>0.09322</td>\n",
       "      <td>0.118644</td>\n",
       "      <td>0.245763</td>\n",
       "      <td>0.186441</td>\n",
       "      <td>0.084746</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.161017</td>\n",
       "      <td>0.372881</td>\n",
       "      <td>0.389831</td>\n",
       "      <td>0.330508</td>\n",
       "      <td>0.313559</td>\n",
       "      <td>0.245763</td>\n",
       "      <td>0.118644</td>\n",
       "      <td>0.135593</td>\n",
       "      <td>0.220339</td>\n",
       "      <td>0.169492</td>\n",
       "      <td>0.059322</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.161017</td>\n",
       "      <td>0.355932</td>\n",
       "      <td>0.305085</td>\n",
       "      <td>0.338983</td>\n",
       "      <td>0.220339</td>\n",
       "      <td>0.169492</td>\n",
       "      <td>0.101695</td>\n",
       "      <td>0.161017</td>\n",
       "      <td>0.245763</td>\n",
       "      <td>0.177966</td>\n",
       "      <td>0.050847</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.211864</td>\n",
       "      <td>0.398305</td>\n",
       "      <td>0.355932</td>\n",
       "      <td>0.288136</td>\n",
       "      <td>0.228814</td>\n",
       "      <td>0.20339</td>\n",
       "      <td>0.084746</td>\n",
       "      <td>0.152542</td>\n",
       "      <td>0.211864</td>\n",
       "      <td>0.186441</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.135593</td>\n",
       "      <td>0.355932</td>\n",
       "      <td>0.271186</td>\n",
       "      <td>0.322034</td>\n",
       "      <td>0.262712</td>\n",
       "      <td>0.186441</td>\n",
       "      <td>0.076271</td>\n",
       "      <td>0.169492</td>\n",
       "      <td>0.194915</td>\n",
       "      <td>0.161017</td>\n",
       "      <td>0.076271</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.186441</td>\n",
       "      <td>0.313559</td>\n",
       "      <td>0.330508</td>\n",
       "      <td>0.313559</td>\n",
       "      <td>0.194915</td>\n",
       "      <td>0.254237</td>\n",
       "      <td>0.101695</td>\n",
       "      <td>0.20339</td>\n",
       "      <td>0.228814</td>\n",
       "      <td>0.177966</td>\n",
       "      <td>0.067797</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.177966</td>\n",
       "      <td>0.40678</td>\n",
       "      <td>0.305085</td>\n",
       "      <td>0.338983</td>\n",
       "      <td>0.211864</td>\n",
       "      <td>0.211864</td>\n",
       "      <td>0.135593</td>\n",
       "      <td>0.144068</td>\n",
       "      <td>0.245763</td>\n",
       "      <td>0.161017</td>\n",
       "      <td>0.042373</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.144068</td>\n",
       "      <td>0.313559</td>\n",
       "      <td>0.322034</td>\n",
       "      <td>0.29661</td>\n",
       "      <td>0.211864</td>\n",
       "      <td>0.237288</td>\n",
       "      <td>0.084746</td>\n",
       "      <td>0.144068</td>\n",
       "      <td>0.220339</td>\n",
       "      <td>0.211864</td>\n",
       "      <td>0.084746</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.20339</td>\n",
       "      <td>0.330508</td>\n",
       "      <td>0.364407</td>\n",
       "      <td>0.322034</td>\n",
       "      <td>0.220339</td>\n",
       "      <td>0.194915</td>\n",
       "      <td>0.101695</td>\n",
       "      <td>0.20339</td>\n",
       "      <td>0.228814</td>\n",
       "      <td>0.169492</td>\n",
       "      <td>0.050847</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.169492</td>\n",
       "      <td>0.389831</td>\n",
       "      <td>0.347458</td>\n",
       "      <td>0.288136</td>\n",
       "      <td>0.271186</td>\n",
       "      <td>0.169492</td>\n",
       "      <td>0.101695</td>\n",
       "      <td>0.169492</td>\n",
       "      <td>0.177966</td>\n",
       "      <td>0.186441</td>\n",
       "      <td>0.042373</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.194915</td>\n",
       "      <td>0.423729</td>\n",
       "      <td>0.372881</td>\n",
       "      <td>0.279661</td>\n",
       "      <td>0.262712</td>\n",
       "      <td>0.169492</td>\n",
       "      <td>0.101695</td>\n",
       "      <td>0.152542</td>\n",
       "      <td>0.220339</td>\n",
       "      <td>0.152542</td>\n",
       "      <td>0.050847</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.228814</td>\n",
       "      <td>0.347458</td>\n",
       "      <td>0.364407</td>\n",
       "      <td>0.330508</td>\n",
       "      <td>0.20339</td>\n",
       "      <td>0.211864</td>\n",
       "      <td>0.067797</td>\n",
       "      <td>0.161017</td>\n",
       "      <td>0.177966</td>\n",
       "      <td>0.144068</td>\n",
       "      <td>0.042373</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.169492</td>\n",
       "      <td>0.372881</td>\n",
       "      <td>0.398305</td>\n",
       "      <td>0.288136</td>\n",
       "      <td>0.262712</td>\n",
       "      <td>0.237288</td>\n",
       "      <td>0.101695</td>\n",
       "      <td>0.177966</td>\n",
       "      <td>0.271186</td>\n",
       "      <td>0.228814</td>\n",
       "      <td>0.067797</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.20339</td>\n",
       "      <td>0.347458</td>\n",
       "      <td>0.29661</td>\n",
       "      <td>0.364407</td>\n",
       "      <td>0.254237</td>\n",
       "      <td>0.169492</td>\n",
       "      <td>0.101695</td>\n",
       "      <td>0.144068</td>\n",
       "      <td>0.211864</td>\n",
       "      <td>0.194915</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.177966</td>\n",
       "      <td>0.389831</td>\n",
       "      <td>0.355932</td>\n",
       "      <td>0.372881</td>\n",
       "      <td>0.271186</td>\n",
       "      <td>0.288136</td>\n",
       "      <td>0.110169</td>\n",
       "      <td>0.144068</td>\n",
       "      <td>0.220339</td>\n",
       "      <td>0.161017</td>\n",
       "      <td>0.050847</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Age body weight       BMI Creatinine clearance       Alb        TO  \\\n",
       "0   0.161017    0.491525  0.389831             0.389831  0.271186  0.228814   \n",
       "1   0.186441    0.372881  0.415254             0.398305  0.237288  0.271186   \n",
       "2    0.20339    0.389831  0.279661             0.322034  0.254237  0.220339   \n",
       "3   0.186441    0.423729  0.322034             0.398305  0.254237  0.220339   \n",
       "4   0.161017    0.355932  0.305085             0.313559  0.313559  0.254237   \n",
       "5   0.127119    0.415254  0.338983             0.305085   0.29661  0.211864   \n",
       "6   0.161017    0.372881  0.389831             0.330508  0.313559  0.245763   \n",
       "7   0.161017    0.355932  0.305085             0.338983  0.220339  0.169492   \n",
       "8   0.211864    0.398305  0.355932             0.288136  0.228814   0.20339   \n",
       "9   0.135593    0.355932  0.271186             0.322034  0.262712  0.186441   \n",
       "10  0.186441    0.313559  0.330508             0.313559  0.194915  0.254237   \n",
       "11  0.177966     0.40678  0.305085             0.338983  0.211864  0.211864   \n",
       "12  0.144068    0.313559  0.322034              0.29661  0.211864  0.237288   \n",
       "13   0.20339    0.330508  0.364407             0.322034  0.220339  0.194915   \n",
       "14  0.169492    0.389831  0.347458             0.288136  0.271186  0.169492   \n",
       "15  0.194915    0.423729  0.372881             0.279661  0.262712  0.169492   \n",
       "16  0.228814    0.347458  0.364407             0.330508   0.20339  0.211864   \n",
       "17  0.169492    0.372881  0.398305             0.288136  0.262712  0.237288   \n",
       "18   0.20339    0.347458   0.29661             0.364407  0.254237  0.169492   \n",
       "19  0.177966    0.389831  0.355932             0.372881  0.271186  0.288136   \n",
       "\n",
       "          T1        T2        T3        T4        T5        T6        T7  \n",
       "0    0.09322  0.144068  0.194915  0.194915  0.067797       0.0       0.0  \n",
       "1   0.067797  0.169492  0.186441  0.169492  0.050847       0.0       0.0  \n",
       "2   0.059322  0.161017  0.144068  0.177966  0.050847       0.0       0.0  \n",
       "3   0.135593  0.144068  0.211864  0.161017  0.084746       0.0  0.008475  \n",
       "4   0.050847  0.127119  0.211864  0.152542  0.067797  0.008475       0.0  \n",
       "5    0.09322  0.118644  0.245763  0.186441  0.084746       0.0       0.0  \n",
       "6   0.118644  0.135593  0.220339  0.169492  0.059322  0.008475  0.008475  \n",
       "7   0.101695  0.161017  0.245763  0.177966  0.050847       0.0       0.0  \n",
       "8   0.084746  0.152542  0.211864  0.186441  0.033898  0.008475  0.008475  \n",
       "9   0.076271  0.169492  0.194915  0.161017  0.076271       0.0       0.0  \n",
       "10  0.101695   0.20339  0.228814  0.177966  0.067797       0.0       0.0  \n",
       "11  0.135593  0.144068  0.245763  0.161017  0.042373       0.0       0.0  \n",
       "12  0.084746  0.144068  0.220339  0.211864  0.084746       0.0  0.008475  \n",
       "13  0.101695   0.20339  0.228814  0.169492  0.050847       0.0       0.0  \n",
       "14  0.101695  0.169492  0.177966  0.186441  0.042373       0.0       0.0  \n",
       "15  0.101695  0.152542  0.220339  0.152542  0.050847  0.008475       0.0  \n",
       "16  0.067797  0.161017  0.177966  0.144068  0.042373       0.0       0.0  \n",
       "17  0.101695  0.177966  0.271186  0.228814  0.067797  0.008475       0.0  \n",
       "18  0.101695  0.144068  0.211864  0.194915  0.033898       0.0       0.0  \n",
       "19  0.110169  0.144068  0.220339  0.161017  0.050847       0.0       0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#TEIC-module 5.3 (permutation importance)\n",
    "\n",
    "#enter the filepath3 and 4 before running the program\n",
    "\n",
    "filepath1 =\"../dataset/TEIC_data(expert_non-ICU)(TEIC-table3).csv\"#filepath to the dataset (TEIC-table3)\n",
    "filepath2 =\"../weight parameter/external validation (expertML)\"#filepath to the folder containing weight parameters\n",
    "filepath3 =\"\"#filepath to the folder where to save the result (csv)\n",
    "filepath4 =\"\"#filepath to the folder where to save the result (figure)\n",
    "\n",
    "\n",
    "from pyparsing.helpers import Keyword\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics as st\n",
    "\n",
    "def one_hot_encoding(x):\n",
    "    unique = list(np.unique(x))\n",
    "    for i in unique:\n",
    "        x = np.where(x == i, unique.index(i), x)\n",
    "    x = np.ravel(x).astype(int)\n",
    "    one_hot = np.eye(len(np.unique(x)))[x]\n",
    "    return unique, one_hot    \n",
    "\n",
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x.T\n",
    "        x = x - np.max(x, axis=0)\n",
    "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "        return y.T \n",
    "\n",
    "    x = x - np.max(x) #oveflow\n",
    "    return np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.res\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))  \n",
    "\n",
    "def sigmoid_grad(x):\n",
    "    return (1.0 - sigmoid(x)) * sigmoid(x)\n",
    "\n",
    "\n",
    "def change_key(dic, ls):#change key in dic to ls\n",
    "    dic_new = {}\n",
    "    for value, key in zip(dic.values(),ls):\n",
    "        dic_new[key]=value\n",
    "    return dic_new\n",
    "\n",
    "def df_list_to_value(df,ln):#ln = the length of the list\n",
    "    for i in df.columns.values:\n",
    "        ls = df.at[0,i]\n",
    "        for j in range(ln):\n",
    "            df.at[j,i] = ls[j]\n",
    "    return df\n",
    "\n",
    "class TwoLayerNet:\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "    def predict(self, x):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "    \n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        return y\n",
    "        \n",
    "    # x:input, t:output\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        \n",
    "        return cross_entropy_error(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "        \n",
    "    # x:input, t:output\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads\n",
    "        \n",
    "    def gradient(self, x, t):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        grads = {}\n",
    "        \n",
    "        batch_num = x.shape[0]\n",
    "        \n",
    "        # forward\n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        # backward\n",
    "        dy = (y - t) / batch_num\n",
    "        grads['W2'] = np.dot(z1.T, dy)\n",
    "        grads['b2'] = np.sum(dy, axis=0)\n",
    "        \n",
    "        dz1 = np.dot(dy, W2.T)\n",
    "        da1 = sigmoid_grad(a1) * dz1\n",
    "        grads['W1'] = np.dot(x.T, da1)\n",
    "        grads['b1'] = np.sum(da1, axis=0)\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def permuted(self, x):\n",
    "        dic = {}\n",
    "        for i in range(x.shape[1]):\n",
    "            permuted_np = x.copy()\n",
    "            permuted_np[:,i] = np.random.permutation(x[:,i])\n",
    "            dic[i]=permuted_np\n",
    "        return(dic)\n",
    "\n",
    "    def pimp(self, x, t, n):\n",
    "        permuted_scores = {}\n",
    "        score_difference = {}\n",
    "        # base score\n",
    "        base_score = self.accuracy(x, t)\n",
    "        # score after shuffling\n",
    "        for i in range(x.shape[1]):#empty list for each parameter\n",
    "            permuted_scores[i]=[]\n",
    "            score_difference[i]=[]\n",
    "\n",
    "        for j in range(n):\n",
    "            self.permuted(x)\n",
    "            for column_id, permuted_x_param in self.permuted(x).items():\n",
    "                permuted_score = self.accuracy(permuted_x_param, t)\n",
    "                permuted_scores[column_id].append(permuted_score)\n",
    "                score_difference[column_id].append(base_score-permuted_score)\n",
    "        return base_score, permuted_scores, score_difference\n",
    "    \n",
    "def score_difference_statistics(dic):\n",
    "    dic_stat_mean = {}\n",
    "    dic_stat_stdev = {}\n",
    "    for column_id, param in dic.items():\n",
    "        dic_stat_mean[column_id] = st.mean(param)\n",
    "        dic_stat_stdev[column_id] = st.stdev(param)\n",
    "    return dic_stat_mean,dic_stat_stdev\n",
    "\n",
    "data = pd.read_csv(filepath1)#experts' data\n",
    "\n",
    "param_list1=[\"Age\",\"body weight\",\"BMI\",\"Creatinine clearance\",\"Alb\",\"TO\"]\n",
    "param_list2 = [\"T1\",\"T2\",\"T3\",\"T4\",\"T5\",\"T6\",\"T7\"]\n",
    "param_list3 = [\"index\"]\n",
    "param_whole = param_list1+param_list2\n",
    "\n",
    "parameter1 = np.array(data[param_list1]).astype(np.float32) #patameter1 (numerical)\n",
    "parameter2 = np.array(data[param_list2]) #patameter2 (binary)\n",
    "ID = np.array(data[param_list3])\n",
    "loading = np.array(data[[\"loading dose\"]])\n",
    "maintenance = np.array(data[[\"maintenance dose\"]])\n",
    "TDM = np.array(data[[\"TDM\",\"TDM exclusion criteria\"]])\n",
    "\n",
    "#transform loading to one-hot encoding\n",
    "ll, loading_one_hot = one_hot_encoding(loading)\n",
    "\n",
    "#transform maintenance to one-hot encoding\n",
    "lm, maintenance_one_hot = one_hot_encoding(maintenance)\n",
    "\n",
    "sc = StandardScaler()\n",
    "sc.fit(parameter1)\n",
    "parameter1_scaler = sc.transform(parameter1)\n",
    "parameter_scaler = np.concatenate([parameter1_scaler, parameter2], 1)\n",
    "\n",
    "#loading\n",
    "network_loading = TwoLayerNet(input_size=len(param_whole), hidden_size=15, output_size=len(np.unique(loading_model)))\n",
    "\n",
    "#paramter incorporation\n",
    "for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "    network_loading.params[key] = np.load(str(filepath2)+\"/loading.param({}_whole_non-ICU_lr_0.1).npy\".format(key))\n",
    "\n",
    "pred_loading_one_hot = network_loading.predict(parameter_scaler)\n",
    "pm_score_diff_ld = network_loading.pimp(parameter_scaler,loading_one_hot,20)[2]\n",
    "pm_score_diff_ld = change_key(pm_score_diff_ld, param_whole) #change key (1,2,3...) to parameter names\n",
    "\n",
    "print(\"permuted importance(loading); mean and stdev\")\n",
    "print(score_difference_statistics(pm_score_diff_ld))\n",
    "\n",
    "#graph\n",
    "plt.subplots_adjust(wspace=0.4, hspace=1)#graph adjust\n",
    "\n",
    "\n",
    "x = [\"Age\",\"BW\",\"BMI\",\"CLCR\",\"Alb\",\"TO\",\"T1\",\"T2\",\"T3\",\"T4\",\"T5\",\"T6\",\"T7\"]\n",
    "y_ld = [i for i in score_difference_statistics(pm_score_diff_ld)[0].values()]\n",
    "err_ld = [i for i in score_difference_statistics(pm_score_diff_ld)[1].values()]\n",
    "\n",
    "plt.subplot(211)\n",
    "plt.bar(x, y_ld, width=0.6, linewidth = 2, color = \"violet\", yerr=err_ld, capsize=4)\n",
    "plt.xticks(x, fontsize=5)\n",
    "plt.ylim(0,1.0)\n",
    "plt.xlabel(\"parameter\")\n",
    "plt.ylabel(\"feature importance \\n(loading dose)\")\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "df_ld = pd.DataFrame([pm_score_diff_ld])\n",
    "df_list_to_value(df_ld,len(df_ld.iat[0,0]))\n",
    "display(df_ld)\n",
    "\n",
    "df_ld.to_csv(str(filepath3)+\"/feature_importance_ld_non-ICU.csv\", index=False)\n",
    "\n",
    "#maintenance\n",
    "network_maintenance = TwoLayerNet(input_size=len(param_whole), hidden_size=15, output_size=len(np.unique(maintenance_model)))\n",
    "\n",
    "#parameter incorporation\n",
    "for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "    network_maintenance.params[key] = np.load(str(filepath2)+\"/maintenance.param({}_whole_non-ICU_lr_0.1).npy\".format(key))\n",
    "\n",
    "pred_maintenance_one_hot = network_maintenance.predict(parameter_scaler)\n",
    "pm_score_diff_mn = network_maintenance.pimp(parameter_scaler,maintenance_one_hot,20)[2]\n",
    "pm_score_diff_mn = change_key(pm_score_diff_mn, param_whole) #change key (1,2,3...) to parameter names\n",
    "\n",
    "\n",
    "print(\"permuted importance (maintenance); mean and stdev\")\n",
    "print(score_difference_statistics(pm_score_diff_mn))\n",
    "\n",
    "#graph\n",
    "x = [\"Age\",\"BW\",\"BMI\",\"CLCR\",\"Alb\",\"TO\",\"T1\",\"T2\",\"T3\",\"T4\",\"T5\",\"T6\",\"T7\"]\n",
    "y_mn = [i for i in score_difference_statistics(pm_score_diff_mn)[0].values()]\n",
    "err_mn = [i for i in score_difference_statistics(pm_score_diff_mn)[1].values()]\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.bar(x, y_mn, width=0.6, linewidth = 2, color = \"turquoise\", yerr=err_ld, capsize=4)\n",
    "plt.xticks(x, fontsize=5)\n",
    "plt.ylim(0,1.0)\n",
    "plt.xlabel(\"parameter\")\n",
    "plt.ylabel(\"feature importance \\n(maintenance dose)\")\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "df_mn = pd.DataFrame([pm_score_diff_mn])\n",
    "df_list_to_value(df_mn,len(df_mn.iat[0,0]))\n",
    "display(df_mn)\n",
    "\n",
    "df_mn.to_csv(str(filepath3)+\"/feature_importance_mn_non-ICU.csv\", index=False)\n",
    "\n",
    "plt.savefig(str(filepath4)+\"/TEIC_figureC.png\",format=\"png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e368d2aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
